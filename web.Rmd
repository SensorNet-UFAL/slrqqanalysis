---
title: "Systematic Literature Review with quantitative analysis of scientific production: Scientometrics using R"
author: "Randy Quindai"
date: "August, 2018"
output: 
  html_document:
    toc: true
    fig_caption: true
    df_print: paged
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*LaCCAN, Brasil | *
*Federal University of Alagoas*

## Introduction
The project associated to this document can be found on github  at <https://github.com/SensorNet-UFAL/slrqqanalysis>, we are accepting pull requests, please feel free to send us yours recommendations.
All data was tested in this configuration settings:
```{r}
sessionInfo()
```

## Reading Data {.tabset .tabset-fade}

### Bibtex 
There are many ways to read bibtex data in R, we'll list all the different methods we can find out there, or propose new ones.

**Bibliometrix** is the library used in this snippet, *savedrecs.bib* is the bibtex file extracted.
```{r bibliometrix, message=FALSE, warning=FALSE}
library(bibliometrix)
D <- readFiles("savedrecs.bib")
M <- convert2df(D, dbsource = "isi", format = "bibtex")
#One can explore all the field tags imported from bib file
M
```
Based on <https://cran.r-project.org/web/packages/bibliometrix/vignettes/bibliometrix-vignette.html>, more details exploring the library bibliometrix can be found there, details of fields <http://www.bibliometrix.org/documents/Field_Tags_bibliometrix.pdf>.

### PDF
For reading `.pdf` files, one can take the followings steps:

The use of topic modeling on published articles provides a way to quantify the content of the study. Following snippets will walk you through these findings.

**tm** is the library that we'll explore, all files are in the folder `pdfs`.
```{r, message=FALSE}
library(tm)
# read pdf files from folder pdfs
# Warning: will pop an error if has a number in the pdf file
files <- list.files("pdfs/",pattern = "pdf$")

# defines a function called rpdf
rpdf <- readPDF(control = list(text = "-layout"))

setwd("pdfs/")

# variable my.pdf can be explored using $
# verify and try to extract some information
my.corpus <- Corpus(URISource(files), readerControl = list(reader = rpdf))
my.pdfdtm <- DocumentTermMatrix(my.corpus, control=list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE, stemming = FALSE, removeNumbers = TRUE))
```

## Processing data {.tabset .tabset-fade}

### Bibtex

Using `bibliometrix` package, below gaphic can be visualized.
```{r keyword}
NetMatrix <- biblioNetwork(M, analysis = "co-occurrences", network = "keywords", sep = ";")
net=networkPlot(NetMatrix, normalize="association", weighted=T, n = 30, Title = "Keyword Co-occurrences", type = "fruchterman", size=T,edgesize = 5,labelsize=0.7)
```


Countries collaborating on this set of bibliographic data
```{r}
countries <- metaTagExtraction(M, Field = "AU_CO", sep = ";")
NetMatrix <- biblioNetwork(countries, analysis = "collaboration", network = "countries", sep = ";")
net=networkPlot(NetMatrix, n = dim(NetMatrix)[1], Title = "Country Collaboration", type = "circle", size=TRUE, remove.multiple=FALSE,labelsize=0.7,cluster="none")
```


One can visualize for instance, which articles cite a specific author
```{r}
M[grep("CHEONG E", M$CR),2]
```

Local citations
```{r}
localTC <- localCitations(M, sep = ";")
lapply(localTC, head)
```


Or to obtain the most dominant authors in a set
```{r}
library(bibliometrix)
results <- biblioAnalysis(M, sep = ";")
dominance(results, k=10)
```


<br/>


DE		 Author's keywords
Type `help("conceptualStructure")` to see details about this function.
```{r}
library(bibliometrix)
cs <- conceptualStructure(M, field = "DE", method = "MCA", quali.supp = NULL, quanti.supp = NULL, minDegree = 2, k.max = 5, stemming = FALSE, labelsize = 10, documents = 50)
```


&nbsp;


Create a historigraphic map proposed by E. Garfield to represent a chronological network map os most relevant direct citations resulting from a bibliographic collection.
```{r}
options(width=130)
histResults <- histNetwork(M, min.citations = 10, sep = ".  ")
net <- histPlot(histResults, n=10, size = 10, labelsize=5, size.cex=TRUE, arrowsize = 0.5, color = TRUE)
```


```{r}
library(bibliometrix)
library(reshape2)
library(ggplot2)
kword <- KeywordGrowth(M, Tag = "DE", sep = ";", top = 15, cdf = TRUE)
DF = melt(kword, id='Year')
#timeline keywords ggplot
ggplot(DF,aes(x=Year,y=value, group=variable, shape=variable, colour=variable))+
  geom_point()+geom_line()+ 
  scale_shape_manual(values = 1:15)+
  labs(color="Author Keywords")+
  scale_x_continuous(breaks = seq(min(DF$Year), max(DF$Year), by = 5))+
  scale_y_continuous(breaks = seq(0, max(DF$value), by=10))+
  guides(color=guide_legend(title = "Author Keywords"), shape=FALSE)+
  labs(y="Count", variable="Author Keywords", title = "Author's Keywords Usage Evolution Over Time")+
  theme(text = element_text(size = 10))+
  facet_grid(variable ~ .)
```

### PDF
Using `dplyr`, `ggplot2` and `tidytext` packages, corpus data can be explored to extract information and visualization.

For this, LDA (Latent Dirichlet Allocation) is a great algorithm to extract information from all this data.

Library `ldatuning` to find the best number of topics to represent this sample on a LDA model.
```{r}
# observe the method used, you can choose Gibbs or VEM
library(ldatuning)
library(topicmodels)
tunningresult <- FindTopicsNumber(
my.pdfdtm,
topics = seq(from = 2, to = 15, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = 3L,
verbose = TRUE)
FindTopicsNumber_plot(tunningresult)
```


You can run this snnipet as many times as you'd like, varying the number of CPU cores to suit your needs.

Observe carefully the graph above, it suggests a model with *k* between 12 and 16, we'll choose *k=13*

```{r}
# Observe the value of k and the method used, you have two options (VEM or Gibbs)
# matrix Whether to tidy the beta (per-term-per-topic, default) or gamma (per-document-per-topic) matrix
library(ldatuning)
library(topicmodels)
library(tidytext)
my.pdf.lda <- LDA(my.pdfdtm, k=13, control=list(seed=77), method="Gibbs")
my.pdf.topics <- tidy(my.pdf.lda, matrix = "beta")
# optional: to visualize the data uncomment below
#my.pdf.ap_topics
```


Observe the use of *matrix="beta"*, this is important for our analysis, because, later on we'll analyze how much of the topic is represented on an article.

To a better visualization of the topics, below snnipet gets the top 10 words in each topic.

```{r, message=FALSE}
library(ldatuning)
library(topicmodels)
library(tidytext)
library(dplyr)
library(magrittr)
my.pdf.top_terms <- my.pdf.topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup %>%
  arrange(topic, -beta)
# optional: to visualize the data uncomment below
#my.pdf.aptop_terms
```

Finnaly, the visualization of top 10 words in each topic.
```{r}
library(ldatuning)
library(topicmodels)
library(tidytext)
library(magrittr)
my.pdf.top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill=factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```


Now we are going to explore how those topics spread in the articles content.

```{r}
#resgatar os documentos associados a cada t√≥pico
my.pdf.gamma <- tidy(my.pdf.lda, matrix = "gamma")

#plot data factor topic
my.pdf.gamma %>%
  mutate(title = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title)
```

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(tidytext)
my.pdftidy <- tidy(my.pdfdtm)

my.pdfsentiment <- my.pdftidy %>%
  inner_join(get_sentiments("bing"), by = c(term ="word"))

my.pdfsentiment %>%
count(sentiment, term, wt = count) %>%
ungroup() %>%
filter(n >= 200) %>%
mutate(n =ifelse(sentiment == "negative", -n, n))%>%
mutate(term=reorder(term,n)) %>%
ggplot(aes(term, n, fill = sentiment)) +
geom_bar(stat="identity") +
ylab("Sentiment analysis on this set") +
coord_flip()
```

## Perplexity
Is a measure of how well a probability model predicts a sample, all credits are from http://freerangestats.info/blog/2017/01/05/topic-model-cv, please visit this site for more details.


## Extras

https://www.researchgate.net/post/What_software_do_you_recommend_for_scientometrics_studies

https://www.mindmeister.com/39583892/research-tools-by-nader-ale-ebrahim

http://www.jzus.zju.edu.cn/manuscript.php

https://rpubs.com/chrisbail/nlp_and_topic_models

